---
title: 单词接龙
date: 2021-01-12
tags:
  - leetcode
summary: 刷算法,向前进
author: Atrist
---

## 参考资料

1. [leetcode 高级算法](https://leetcode-cn.com/leetbook/detail/top-interview-questions-hard/)
2. [单词接龙](https://leetcode-cn.com/problems/word-ladder/description/)
## 单词接龙
给定两个单词（beginWord 和 endWord）和一个字典，找到从 beginWord 到 endWord 的最短转换序列的长度。转换需遵循如下规则：
1. 每次转换只能改变一个字母。
2. 转换过程中的中间单词必须是字典中的单词

### 说明:
- 如果不存在这样的转换序列，返回 0。
- 所有单词具有相同的长度。
- 所有单词只由小写字母组成。
- 字典中不存在重复的单词。
- 你可以假设 beginWord 和 endWord 是非空的，且二者不相同。
### 示例 1:
```bash
输入:
beginWord = "hit",
endWord = "cog",
wordList = ["hot","dot","dog","lot","log","cog"]

输出: 5

解释: 一个最短转换序列是 "hit" -> "hot" -> "dot" -> "dog" -> "cog",
     返回它的长度 5。
```
### 示例 2:
```bash
输入:
beginWord = "hit"
endWord = "cog"
wordList = ["hot","dot","dog","lot","log"]

输出: 0

解释: endWord "cog" 不在字典中，所以无法进行转换。
```
## 方法一：广度优先搜索 + 优化建图
### 思路
本题要求的是**最短转换序列**的长度，看到最短首先想到的就是**广度优先搜索**。想到**广度优先搜索**自然而然的就能想到图，但是本题并没有直截了当的给出图的模型，因此我们需要把它抽象成图的模型。

我们可以把每个单词都抽象为一个点，如果两个单词可以只改变一个字母进行转换，那么说明他们之间有一条双向边。因此我们只需要把满足转换条件的点相连，就形成了一张**图**。

基于该图，我们以 `beginWord` 为图的起点，以 `endWord` 为终点进行**广度优先搜索**，寻找 `beginWord` 到 `endWord` 的最短路径。

![](./images/127_fig1.png)

### 算法

基于上面的思路我们考虑如何编程实现。

首先为了方便表示，我们先给每一个单词标号，即给每个单词分配一个 `id`。创建一个由单词 `word` 到 `id` 对应的映射 `wordId`，并将 `beginWord` 与 `wordList` 中所有的单词都加入这个映射中。之后我们检查 `endWord` 是否在该映射内，若不存在，则输入无解。我们可以使用**哈希表**实现上面的映射关系。

然后我们需要建图，依据朴素的思路，我们可以枚举每一对单词的组合，判断它们是否恰好相差一个字符，以判断这两个单词对应的节点是否能够相连。但是这样效率太低，我们可以优化建图。

具体地，我们可以创建虚拟节点。对于单词 `hit`，我们创建三个虚拟节点 `*it`、`h*t`、`hi*`，并让 `hit` 向这三个虚拟节点分别连一条边即可。如果一个单词能够转化为 `hit`，那么该单词必然会连接到这三个虚拟节点之一。对于每一个单词，我们枚举它连接到的虚拟节点，把该单词对应的 `id` 与这些虚拟节点对应的 `id` 相连即可。

最后我们将起点加入队列开始广度优先搜索，当搜索到终点时，我们就找到了最短路径的长度。注意因为添加了虚拟节点，所以我们得到的距离为实际最短路径长度的两倍。同时我们并未计算起点对答案的贡献，所以我们应当返回距离的一半再加一的结果。
### 代码
```js
/**
 * @param {string} beginWord
 * @param {string} endWord
 * @param {string[]} wordList
 * @return {number}
 */
var ladderLength = function (beginWord, endWord, wordList) {
  const wordId = new Map();
  let edge = [];
  let nodeNum = 0;
  const addEdge = (word) => {
    addWord(word);
    let id1 = wordId.get(word);
    let array = word.split("");
    let length = array.length;
    for (let i = 0; i < length; i++) {
      let temp = array[i];
      array[i] = "*";
      const newWord = array.join("");
      addWord(newWord);
      let id2 = wordId.get(newWord);
      edge[id1].push(id2);
      edge[id2].push(id1);
      array[i] = temp;
    }
  };
  const addWord = (word) => {
    if (!wordId.has(word)) {
      wordId.set(word, nodeNum++);
      edge.push([]);
    }
  };
  for (let word of wordList) {
    addEdge(word);
  }
  addEdge(beginWord);
  if (!wordId.has(endWord)) return 0;
  let dis = new Array(nodeNum).fill(Number.MAX_VALUE);
  let beginId = wordId.get(beginWord),
    endId = wordId.get(endWord);
  dis[beginId] = 0;
  let que = [];
  que.push(beginId);
  while (!(que.length === 0)) {
    let x = que.shift();
    if (x === endId) return dis[endId] / 2 + 1;
    for (let it of edge[x]) {
      if (dis[it] === Number.MAX_VALUE) {
        dis[it] = dis[x] + 1;
        que.push(it);
      }
    }
  }
  return 0;
};
```
### 复杂度分析
- 时间复杂度：$O(N \times C^2)$。其中 N 为 wordList 的长度，C 为列表中单词的平均长度。

  - 建图过程中，对于每一个单词，我们需要枚举它连接到的所有虚拟节点，时间复杂度为 O(C)，将这些单词加入到哈希表中，时间复杂度为 $O(N \times C)$，因此总时间复杂度为 $O(N \times C)$。
  - 广度优先搜索的时间复杂度最坏情况下是 $O(N \times C)$。每一个单词需要拓展出 $O(C)$ 个虚拟节点，因此节点数 $O(N \times C)$。

- 空间复杂度：$O(N \times C^2)$。其中 N 为 wordList 的长度，C 为列表中单词的平均长度。哈希表中包含 $O(N \times C)$ 个节点，每个节点占用空间 O(C)，因此总的空间复杂度为 $O(N \times C^2)$。
## 方法二：双向广度优先搜索
### 思路及解法

根据给定字典构造的图可能会很大，而广度优先搜索的搜索空间大小依赖于每层节点的分支数量。假如每个节点的分支数量相同，搜索空间会随着层数的增长指数级的增加。考虑一个简单的二叉树，每一层都是满二叉树的扩展，节点的数量会以 22 为底数呈指数增长。

如果使用两个同时进行的广搜可以有效地减少搜索空间。一边从 `beginWord` 开始，另一边从 `endWord` 开始。我们每次从两边各扩展一层节点，当发现某一时刻两边都访问过同一顶点时就停止搜索。这就是双向广度优先搜索，它可以可观地减少搜索空间大小，从而提高代码运行效率。

![](./images/127_fig2.png)

### 代码
```js
/**
 * @param {string} beginWord
 * @param {string} endWord
 * @param {string[]} wordList
 * @return {number}
 */
var ladderLength = function (beginWord, endWord, wordList) {
  const wordId = new Map();
  let edge = [];
  let nodeNum = 0;
  const addEdge = (word) => {
    addWord(word);
    let id1 = wordId.get(word);
    let array = word.split("");
    let length = array.length;
    for (let i = 0; i < length; i++) {
      let temp = array[i];
      array[i] = "*";
      const newWord = array.join("");
      addWord(newWord);
      let id2 = wordId.get(newWord);
      edge[id1].push(id2);
      edge[id2].push(id1);
      array[i] = temp;
    }
  };
  const addWord = (word) => {
    if (!wordId.has(word)) {
      wordId.set(word, nodeNum++);
      edge.push([]);
    }
  };
  for (let word of wordList) {
    addEdge(word, wordId);
  }
  addEdge(beginWord, wordId);
  if (!wordId.has(endWord)) {
    return 0;
  }
  let disBegin = new Array(nodeNum).fill(Number.MAX_VALUE);
  let beginId = wordId.get(beginWord);
  disBegin[beginId] = 0;
  queBegin = [beginId];
  let disEnd = new Array(nodeNum).fill(Number.MAX_VALUE);
  let endId = wordId.get(endWord);
  disEnd[endId] = 0;
  queEnd = [endId];
  while (!(queBegin.length === 0) && !(queEnd.length === 0)) {
    let queBeginSize = queBegin.length;
    for (let i = 0; i < queBeginSize; ++i) {
      let nodeBegin = queBegin.shift();
      if (disEnd[nodeBegin] !== Number.MAX_VALUE) {
        return Math.floor((disBegin[nodeBegin] + disEnd[nodeBegin]) / 2) + 1;
      }
      for (let it of edge[nodeBegin]) {
        if (disBegin[it] === Number.MAX_VALUE) {
          disBegin[it] = disBegin[nodeBegin] + 1;
          queBegin.push(it);
        }
      }
    }
    let queEndSize = queEnd.length;
    for (let i = 0; i < queEndSize; i++) {
      let nodeEnd = queEnd.shift();
      if (disBegin[nodeEnd] !== Number.MAX_VALUE) {
        return Math.floor((disBegin[nodeEnd] + disEnd[nodeEnd]) / 2) + 1;
      }
      for (let it of edge[nodeEnd]) {
        if (disEnd[it] === Number.MAX_VALUE) {
          disEnd[it] = disEnd[nodeEnd] + 1;
          queEnd.push(it);
        }
      }
    }
  }
  return 0;
};
```

### 复杂度分析

- 时间复杂度：$O(N \times C^2)。其中 N 为 wordList 的长度，C 为列表中单词的平均长度。

  - 建图过程中，对于每一个单词，我们需要枚举它连接到的所有虚拟节点，时间复杂度为 O(C)，将这些单词加入到哈希表中，时间复杂度为 $O(N \times C)$，因此总时间复杂度为 $O(N \times C)$。
  - 双向广度优先搜索的时间复杂度最坏情况下是 $O(N \times C)$。每一个单词需要拓展出 O(C) 个虚拟节点，因此节点数 $O(N \times C)$。

-  空间复杂度：$O(N \times C^2)$。其中 N 为 wordList 的长度，C 为列表中单词的平均长度。哈希表中包含 $O(N \times C)$ 个节点，每个节点占用空间 O(C)，因此总的空间复杂度为 $O(N \times C^2)$。


